from flask import Flask, request, jsonify
from pyspark.sql import SparkSession
from pyspark.ml import PipelineModel
import os
from flask_cors import CORS
from pyspark.sql import functions as F
from tensorflow.keras.models import load_model
import numpy as np
from PIL import Image
import base64
import io
from io import BytesIO
import binascii

os.environ['PYSPARK_PYTHON'] = 'python'
os.environ["TF_ENABLE_ONEDNN_OPTS"] = "0"
app = Flask(__name__)
app.config['MAX_CONTENT_LENGTH'] = 16 * 1024 * 1024  # 16MB, Ä‘iá»u chá»‰nh theo nhu cáº§u

CORS(app)

# Khá»Ÿi táº¡o Spark session
print("ğŸ”„ Äang khá»Ÿi táº¡o Spark session...")
spark = SparkSession.builder \
    .appName("StrokeRiskPrediction") \
    .config("spark.executor.memory", "2g") \
    .config("spark.driver.memory", "2g") \
    .config("spark.executor.cores", "1") \
    .config("spark.python.worker.faulthandler.enabled", "true") \
    .config("spark.sql.execution.pyspark.udf.faulthandler.enabled", "true") \
    .getOrCreate()

# spark = SparkSession.builder.appName("StrokeRiskPrediction").getOrCreate()
print("âœ… Spark session Ä‘Ã£ Ä‘Æ°á»£c khá»Ÿi táº¡o.")

# ÄÆ°á»ng dáº«n tá»›i model
current_dir = os.path.dirname(os.path.abspath(__file__))
model_path = os.path.join(current_dir, "models", "Model_LogisticRegression")

# Load PipelineModel thay vÃ¬ LogisticRegressionModel
print(f"ğŸ”„ Äang load model Logistic RegressionModel tá»«: {model_path}")
model = PipelineModel.load(model_path)
print("âœ… Model Logistic RegressionModel Ä‘Ã£ Ä‘Æ°á»£c load thÃ nh cÃ´ng.")

# ÄÆ°á»ng dáº«n model VGG16
vgg_model_path = os.path.join(current_dir, "models", "vgg16_model.h5")
print(f"ğŸ”„ Äang load model VGG16 tá»«: {vgg_model_path}")
vgg_model = load_model(vgg_model_path)
print("âœ… Model VGG16 Ä‘Ã£ Ä‘Æ°á»£c load thÃ nh cÃ´ng.")

def preprocess_image(pil_image):
    try:
        image = pil_image.convert("RGB").resize((224, 224))
        image_array = np.array(image) / 255.0
        return np.expand_dims(image_array, axis=0)
    except Exception as e:
        raise ValueError(f"Error processing image: {str(e)}")

@app.route('/api/predict', methods=['POST'])
def predict():
    print("\nğŸ“¥ Nháº­n yÃªu cáº§u dá»± Ä‘oÃ¡n tá»« client...")
    data = request.json
    if not data:
        print("âŒ KhÃ´ng nháº­n Ä‘Æ°á»£c dá»¯ liá»‡u Ä‘áº§u vÃ o.")
        return jsonify({"error": "No input data received"}), 400
    print("Dá»¯ liá»‡u nháº­n Ä‘Æ°á»£c tá»« client:", data)

    try:
        input_columns = ['Sex','GeneralHealth','HeightInMeters','WeightInKilograms','BMI',
        'HadHeartAttack','HadAngina','HadStroke','HadAsthma','HadSkinCancer','HadCOPD',
        'HadDepressiveDisorder','HadKidneyDisease','HadArthritis','DeafOrHardOfHearing',
        'BlindOrVisionDifficulty','DifficultyConcentrating','DifficultyWalking',
        'DifficultyDressingBathing','DifficultyErrands','SmokerStatus','ECigaretteUsage',
        'ChestScan','AlcoholDrinkers','HIVTesting','FluVaxLast12','PneumoVaxEver',
        'HighRiskLastYear','CovidPos','AgeMin','AgeMax','TetanusLast10TdapIndex',
        'HadDiabetesIndex']

        # Táº¡o Spark DataFrame tá»« dá»¯ liá»‡u Ä‘áº§u vÃ o
        df = spark.createDataFrame([[data[col] for col in input_columns]], input_columns)
        print("âœ… Dá»¯ liá»‡u Ä‘Ã£ Ä‘Æ°á»£c chuyá»ƒn thÃ nh DataFrame.")
        print("âœ… Schema DataFrame:", df.printSchema())
        print("âœ… CÃ¡c cá»™t trong DataFrame:", df.columns)
        print("âœ… CÃ¡c cá»™t model yÃªu cáº§u:", model.stages[-1].featuresCol if hasattr(model.stages[-1], 'featuresCol') else 'KhÃ´ng rÃµ')

        result_row = model.transform(df).take(1)[0]
        spark_prob = float(result_row.probability[1])
        print(f"âœ… Model Logistic RegressionModel dá»± Ä‘oÃ¡n xÃ¡c suáº¥t lÃ : {spark_prob}")

        # ==== 2. Tiá»n xá»­ lÃ½ áº£nh vÃ  dá»± Ä‘oÃ¡n báº±ng VGG16 model ====
        image_base64 = data.get("image_base64", None)
        if not image_base64:
            return jsonify({'error': 'Thiáº¿u áº£nh'}), 400

        try:
            print("âœ… ÄÃ£ nháº­n áº£nh base64")
            
            # Náº¿u chuá»—i cÃ³ prefix nhÆ°: 'data:image/jpeg;base64,...', ta loáº¡i bá» pháº§n Ä‘áº§u
            header_split = image_base64.split(",")
            if len(header_split) == 2:
                image_base64 = header_split[1]

            # Kiá»ƒm tra base64 cÃ³ pháº£i lÃ  há»£p lá»‡
            try:
                image_data = base64.b64decode(image_base64, validate=True)
            except binascii.Error:
                image_data = base64.b64decode(image_base64 + '=' * (-len(image_base64) % 4))  # Padding base64

            # Äá»c áº£nh tá»« bytes
            with Image.open(io.BytesIO(image_data)) as img:
                image = img.convert("RGB")
            print("âœ… áº¢nh Ä‘Ã£ Ä‘Æ°á»£c decode vÃ  Ä‘á»c thÃ nh cÃ´ng")

        except Exception as e:
            print("âŒ Lá»—i xá»­ lÃ½ áº£nh:", e)
            return jsonify({'error': f'Lá»—i xá»­ lÃ½ áº£nh: {str(e)}'}), 400
        
        processed_image = preprocess_image(image)
        vgg_prob = float(vgg_model.predict(processed_image)[0][0])  # giáº£ sá»­ output lÃ  sigmoid
        print(f"âœ… model VGG16 dá»± Ä‘oÃ¡n xÃ¡c suáº¥t lÃ : {vgg_prob}")

        # ==== 3. Trung bÃ¬nh káº¿t quáº£ ====
        avg_prob = (spark_prob + vgg_prob) / 2
        prediction = 1.0 if avg_prob >= 0.5 else 0.0

        label_map = {0.0: "Low Risk", 1.0: "High Risk"}
        result = {
            "prediction": label_map[prediction],
            "average_probability": round(avg_prob * 100, 2),
            "spark_probability": round(spark_prob * 100, 2),
            "vgg_probability": round(vgg_prob * 100, 2),
        }

        print(f"ğŸ“¤ Káº¿t quáº£ dá»± Ä‘oÃ¡n gá»­i vá» client: {result}")
        return jsonify(result)

    except Exception as e:
        print("âŒ Lá»—i khi dá»± Ä‘oÃ¡n:", e)
        return jsonify({"error": str(e)}), 500

if __name__ == "__main__":
    print("Flask API Ä‘ang cháº¡y...")
    app.run(host='0.0.0.0', port=5000, debug=True)